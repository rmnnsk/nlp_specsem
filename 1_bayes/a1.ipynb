{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Наивный байесовский классификатор, модель Бернулли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A)\n",
    "**Чему равна вероятность $P(w_i ∈ d | d ∈ c_j)$ встретить $i$-ое слово из словаря в случайном\n",
    "документе класса $c_j$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переформулируем этот вопрос для модели Бернулли: \\\n",
    "**Какова вероятность, что i-ый признак документа $d$ равен 1, при условии, что документ принадлежит классу $c_j$.**\n",
    "\\\n",
    "$P(f_i(d) = 1 | d \\in c_j)$, где $f_i$ - функция, возвращающая $i$-ый признак для документа $d$.\\\n",
    "\\\n",
    "По формуле условной вероятности: $$P(f_i(d) = 1 | d \\in c_j) = \\frac{P(f_i(d) = 1, d \\in c_j)}{P(d \\in c_j)}$$\n",
    "Оценим эту вероятность на выборке.\\\n",
    "Это можно сделать по частоте, с которой признак $f_i(d) = 1$ во всех документах выборки.\n",
    "$$P(w_i | c_j) = P(f_i(d) = 1 | d \\in c_j) = \\frac{cnt(f_i(d) = 1, d \\in c_j)}{\\sum_{x \\in {0,1}} cnt(f_i(d) = x, d \\in c_j)}$$\n",
    "$cnt(cond)$ - количество документов из выборки, удовлетворяющих условию $cond$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в тестовой выборки нам встретится слово, которое не встречалалось в обучающей, то вероятность $P(w_i | c_j)$ обнулится для всех $c_j$ и мы не сможем предсказать класс предложения. \n",
    "Поэтому применим сглаживание Лапласа, т.е. добавим фиктивные $\\alpha$ появлений каждого слова в каждом классе.\n",
    "\n",
    "$$P(w_i | c_j) = P(f_i(d) = 1 | d \\in c_j) = \\frac{cnt(f_i(d) = 1, d \\in c_j) + \\alpha}{\\sum_{x \\in {0,1}} (cnt(f_i(d) = x, d \\in c_j) + \\alpha)} = \\frac{cnt(f_i(d) = 1, d \\in c_j) + \\alpha}{\\sum_{x \\in {0,1}} (cnt(f_i(d) = x, d \\in c_j)) + 2*\\alpha}$$\n",
    "Обычно параметр $\\alpha$ выбирается равным 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B)\n",
    "**Вывести $P(d=(k_1, k_2, ... , k_M) | d \\in c_j)$ – вероятность того, что случайный документ $d$, принадлежащий классу $c_j$, будет состоять из $k_1, k_2, ... , k_M$ вхождений слов $v_1, v_2, ... , v_M$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели Бернулли мы учитываем только наличие слова в предложении. Поэтому теряем информацию о порядке слов и количестве вхождений каждого слова и значит $k_i = 0...1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из наивного предположения о независимости аргументов получаем: \n",
    "$$P(d=(k_1, k_2, ... , k_M) | d \\in c_j) = \\prod_{i=1}^{M} P(cnt(w_i) = k_i | d \\in c_j)$$\n",
    "Т.е. произведение вероятностей по каждому слову $w_i$, что оно встречается $k_i$ раз в случайном документе $d$ из класса $c_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем одну вероятность $P(cnt(w_i) = k_i | d \\in c_j)$\n",
    "$$P(cnt(w_i) = k_i | d \\in c_j) = k_i * P(w_i | c_j) + (1 - k_i) * (1 - P(w_i | c_j))$$\n",
    "\n",
    "Тогда исходная вероятность равна:\n",
    "$$P(d=(k_1, k_2, ... , k_M) | d \\in c_j) = \\prod_{i=1}^{M}\\left[k_i * P(w_i | c_j) + (1 - k_i) * (1 - P(w_i | c_j))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C)\n",
    "**Вывести вероятность $P(c_j| d)$, что данный документ принадлежит классу $c_j$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем вместе предыдущие пункты.\n",
    "Для начала применим формулу Байеса. $P(c_j|d) = \\frac{P(d|c_j)P(c_j)}{P(d)}$.\n",
    "В ней нам неизвестны $P(c_j)$ и $P(d)$.\\\n",
    "$P(c_j)$ - вероятность встретить среди всех документов документ класса $c_j$.\\\n",
    "$P(d)$ - вероятность встретить документ $d$ среди всех документов.\\\n",
    "$P(c_j)$ мы можем оценить из обучающей выборки. $P(c_j) = \\frac{N_{c_j}}{N}$, где $N_{c_j}$ - количество документов класса $c_j$, $N$ - общее количество документов.\\\n",
    "Для модели Бернулли, где каждый элемент задаётся M-мерным вектором из 0/1 и без учета порядка, мы можем посчитать $P(d) = \\frac{N_d}{N}$, где $N_d$ - количество векторов $d$ среди всех векторов предложений.\\\n",
    "Но т.к. для каждого класса эта величина одинаковая, то для выявления максимальной вероятности она не нужна. Поэтому для классификации она не используется.\n",
    "\n",
    "В итоге получаем оценку вероятности:\n",
    "$$P(c_j|d) = P(c_j) * \\prod_{i=1}^{M}\\left[k_i * P(w_i | c_j) + (1 - k_i) * (1 - P(w_i | c_j))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D)\n",
    "**Какой класс $c_j$ будет будет выдан для документа d классификатором, если предположить, что $P(c_j)$ и $P(d | c_j)$ заданы? Как можно оценить вероятность ошибки?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При заданных вероятностях $P(c_j)$ и $P(d|c_j)$ мы можем посчитать все вероятности $P(c_j | d)$ и выбрать максимальную.\\\n",
    "$$c_j = argmax_{j} P(c_j|d) = argmax_{j} \\left[P(d|c_j) * P(c_j)\\right]$$\n",
    "\n",
    "При этом, т.к. в $P(d | c_j)$ нам нужно счиать много произведений, то можно выбирать максимум из логарифмов этих величин.\n",
    "\n",
    "Вероятность ошибки можно оценить как \n",
    "$$1 - \\mathbb{E}(max_j(P(d | c_j)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Наивный байесовский классификатор, мультиномиальная модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A)\n",
    "**Чему равна вероятность $P(w_i ∈ d | d ∈ c_j)$ встретить $i$-ое слово из словаря в случайном\n",
    "документе класса $c_j$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В мультиномиальной модели мы рассматриваем документ, как вектор длины словаря, в котором указывается, сколько раз каждое слово встерчалось в предложении.\n",
    "\n",
    "Поэтому оценивать вероятность встретить слово мы будем немного по другому, учитывая количество вхождений всех слов в документы.\n",
    "\n",
    "$$P(w_i \\in d | d \\in c_j) = \\frac{P(w_i \\in d, d \\in c_j)}{P(d \\in c_j)}$$\n",
    "\n",
    "Оценка будет выглядеть следующим образом.\n",
    "$$P(w_i | c_j) = \\frac{cnt(f_i(d), d \\in c_j) + \\alpha}{\\sum_{i}\\sum_{x} (cnt(f_i(d) = x, d \\in c_j) + \\alpha)} = \\frac{cnt(f_i(d), d \\in c_j) + \\alpha}{\\sum_{i}\\sum_{x} (cnt(f_i(d) = x, d \\in c_j)) + |V| * \\alpha}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B)\n",
    "**Вывести $P(d=(k_1, k_2, ... , k_M) | d \\in c_j)$ – вероятность того, что случайный документ $d$, принадлежащий классу $c_j$, будет состоять из $k_1, k_2, ... , k_M$ вхождений слов $v_1, v_2, ... , v_M$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая наивное предположение о независимости аргументов, мы можем рассматривать слова, как случайные величины с полиномиальным распределением, и тогда\n",
    "$$P(d = (k_1, k_2, ..., k_m) | d \\in c_j) = P(|d|) \\frac{|d|!}{k_1!k_2!...k_m!} \\prod_{i=1}^{m}P(w_i|c_j)^{k_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C)\n",
    "**Вывести вероятность $P(c_j| d)$, что данный документ принадлежит классу $c_j$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут аналогично предыдущему пункту после применения формулы Байеса получаем $P(c_j|d) = \\frac{P(d|c_j)P(c_j)}{P(d)}$. \\\n",
    "Оценка для $P(c_j) = \\frac{N_{c_j}}{N}$, где $N_{c_j}$ - количество документов класса $c_j$, $N$ - общее количество документов. \\\n",
    "$P(d)$ можно не оценивать, потому что для классификации не используется.\\\n",
    "Множитель с факториалами не зависит от класса, значит его тоже можно не использовать при классификации.\\\n",
    "Также будем считать, что длины документов встречаются с равной вероятностью.\\\n",
    "И тогда для классификации можно использовать оценку вероятности: $$P(c_j|d) = P(c_j) \\prod_{i=1}^{m}P(w_i|c_j)^{k_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D)\n",
    "**Какой класс $c_j$ будет будет выдан для документа d классификатором, если предположить, что $P(c_j)$ и $P(d | c_j)$ заданы? Как можно оценить вероятность ошибки?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут всё так же, как и в модели Бернулли.\n",
    "$$c_j = argmax_{j} P(c_j|d) = argmax_{j} \\left[P(d|c_j) * P(c_j)\\right]$$\n",
    "\n",
    "\n",
    "Вероятность ошибки можно оценить как \n",
    "$$1 - \\mathbb{E}(max_j(P(d | c_j)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Реализуем два классификатора и сравниваем их на FILMDB датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**а)** Загрузить обучающую выборку в 2 списка – позитивные и негативные отзывы. Чему\n",
    "равна минимальная, максимальная, средняя, медианная длина (в символах) позитивных /\n",
    "негативных отзывов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "pd.set_option('precision', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7520 7480\n"
     ]
    }
   ],
   "source": [
    "train_texts_path = \"./filimdb_evaluation/FILIMDB/train.texts\"\n",
    "train_labels_path = \"./filimdb_evaluation/FILIMDB/train.labels\"\n",
    "\n",
    "dev_texts_path = \"./filimdb_evaluation/FILIMDB/dev.texts\"\n",
    "dev_labels_path = \"./filimdb_evaluation/FILIMDB/dev.labels\"\n",
    "\n",
    "test_texts_path = \"./filimdb_evaluation/FILIMDB/test.texts\"\n",
    "test_labels_path = \"./filimdb_evaluation/FILIMDB/test.labels\"\n",
    "\n",
    "with open(train_texts_path, 'r', encoding='utf-8',) as inp:\n",
    "    train_data = list(map(str.strip, inp.readlines()))\n",
    "with open(train_labels_path, 'r', encoding='utf-8',) as inp:\n",
    "    train_labels = list(map(str.strip, inp.readlines()))\n",
    "\n",
    "train_pos = [train_data[i] for i, lab in enumerate(train_labels) if lab == 'pos']\n",
    "train_neg = [train_data[i] for i, lab in enumerate(train_labels) if lab == 'neg']\n",
    "print(len(train_pos), len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: \n",
      "min_len = 70\n",
      "max_len = 10363\n",
      "mean_len = 1360.801329787234\n",
      "median_len = 997\n",
      "\n",
      "Negative: \n",
      "min_len = 52\n",
      "max_len = 8969\n",
      "mean_len = 1316.356550802139\n",
      "median_len = 981\n"
     ]
    }
   ],
   "source": [
    "def dataset_info(dataset):\n",
    "    lens = [len(text) for text in dataset]\n",
    "    min_len = min(lens)\n",
    "    max_len = max(lens)\n",
    "    mean_len = sum(lens) / len(lens)\n",
    "    median_len = sorted(lens)[len(lens) // 2]\n",
    "\n",
    "    print(f\"min_len = {min_len}\")\n",
    "    print(f\"max_len = {max_len}\")\n",
    "    print(f\"mean_len = {mean_len}\")\n",
    "    print(f\"median_len = {median_len}\")\n",
    "    \n",
    "print(\"Positive: \")\n",
    "dataset_info(train_pos)\n",
    "print()\n",
    "print(\"Negative: \")\n",
    "dataset_info(train_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Сделать предобработку. Перевести отзывы в нижний регистр. Вставить пробелы вокруг\n",
    "всех символов, не являющихся цифрами или буквами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I gave this movie a 10 because it needed to be rewarded for its scary elements and actors AND my god the enging! The thing is I don't want to tell anyone anything about the acting or story because it will ruin the movie. But I will recommend that you go straight to your nearest moviestore right now and rent it! (Don't forget popcorn!)\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    remove_tags = re.compile('<.*?>')\n",
    "    text = re.sub(remove_tags, '', text)\n",
    "    text = ''.join(sym if (sym.isalnum() or sym in (\" \", \"'\")) else f\" {sym} \" for sym in text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# print(preprocessing(train_neg[0]))\n",
    "train_neg = list(map(preprocessing, train_neg))\n",
    "train_pos = list(map(preprocessing, train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i gave this movie a 10 because it needed to be rewarded for its scary elements and actors and my god the enging !  the thing is i don't want to tell anyone anything about the acting or story because it will ruin the movie .  but i will recommend that you go straight to your nearest moviestore right now and rent it !   ( don't forget popcorn !  ) \n"
     ]
    }
   ],
   "source": [
    "print(train_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Сделать токенизацию – то есть представить каждый отзыв в виде списка токенов (слов,\n",
    "чисел, знаков пунктуации). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    \"\"\"\n",
    "        arg: list(np.array) of texts\n",
    "        return: np.array of tokenized texts(np.array)\n",
    "    \"\"\"\n",
    "    tokenizer = re.compile(r\"-?\\d*[.,]?\\d+|[?'\\w]+|\\S\", re.MULTILINE | re.IGNORECASE)\n",
    "    return list(map(lambda doc: list(tokenizer.findall(doc)), dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = tokenize_dataset(train_pos)\n",
    "train_neg = tokenize_dataset(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'gave', 'this', 'movie', 'a', '10', 'because', 'it', 'needed', 'to', 'be', 'rewarded', 'for', 'its', 'scary', 'elements', 'and', 'actors', 'and', 'my', 'god', 'the', 'enging', '!', 'the', 'thing', 'is', 'i', \"don't\", 'want', 'to', 'tell', 'anyone', 'anything', 'about', 'the', 'acting', 'or', 'story', 'because', 'it', 'will', 'ruin', 'the', 'movie', '.', 'but', 'i', 'will', 'recommend', 'that', 'you', 'go', 'straight', 'to', 'your', 'nearest', 'moviestore', 'right', 'now', 'and', 'rent', 'it', '!', '(', \"don't\", 'forget', 'popcorn', '!', ')']\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно, делать одинаковую предобработку, чтобы не появлялся неожиданный мусор в виде знаков препинания и других любых знаков, такой как слова с приклееными запятыми. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Построить 2 Питоновских словаря слово→частота с частотами каждого слов в\n",
    "позитивных и негативных отзывах. Для позитивных и негативных отзывов распечатайте по\n",
    "30 самых частотных слов и их частоты. Распечатайте 30 слов с максимальными и 30 слов с\n",
    "минимальными наивными байесовскими весами "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7520 7480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71104"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ind = {}\n",
    "ind_to_word = {}\n",
    "free_ind = 0\n",
    "\n",
    "print(len(train_pos), len(train_neg))\n",
    "\n",
    "pos_words = defaultdict(int)\n",
    "word_in_pos_text = defaultdict(int)\n",
    "neg_words = defaultdict(int)\n",
    "word_in_neg_text = defaultdict(int)\n",
    "cnt_pos = 0\n",
    "cnt_neg = 0\n",
    "for text in train_pos:\n",
    "    been = set()\n",
    "    for word in text:\n",
    "        if word not in word_to_ind:\n",
    "            word_to_ind[word] = free_ind\n",
    "            ind_to_word[free_ind] = word\n",
    "            free_ind += 1\n",
    "        if word not in been:\n",
    "            ind = word_to_ind[word]\n",
    "            word_in_pos_text[ind] += 1\n",
    "            been.add(word)\n",
    "        pos_words[word_to_ind[word]] += 1\n",
    "        cnt_pos += 1\n",
    "       \n",
    "bigrams = False\n",
    "\n",
    "if bigrams:\n",
    "    for text in train_pos:\n",
    "        been = set()\n",
    "        bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "        for word in bigrams:\n",
    "            if word not in word_to_ind:\n",
    "                word_to_ind[word] = free_ind\n",
    "                ind_to_word[free_ind] = word\n",
    "                free_ind += 1\n",
    "            if word not in been:\n",
    "                ind = word_to_ind[word]\n",
    "                word_in_pos_text[ind] += 1\n",
    "                been.add(word)\n",
    "            pos_words[word_to_ind[word]] += 1\n",
    "            cnt_pos += 1\n",
    "    \n",
    "for text in train_neg:\n",
    "    been = set()\n",
    "    for word in text:\n",
    "        if word not in word_to_ind:\n",
    "            word_to_ind[word] = free_ind\n",
    "            ind_to_word[free_ind] = word\n",
    "            free_ind += 1\n",
    "        if word not in been:\n",
    "            ind = word_to_ind[word]\n",
    "            word_in_neg_text[ind] += 1\n",
    "            been.add(word)\n",
    "        neg_words[word_to_ind[word]] += 1\n",
    "        cnt_neg += 1\n",
    "\n",
    "if bigrams:\n",
    "    for text in train_neg:\n",
    "        been = set()\n",
    "        bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "        for word in bigrams:\n",
    "            if word not in word_to_ind:\n",
    "                word_to_ind[word] = free_ind\n",
    "                ind_to_word[free_ind] = word\n",
    "                free_ind += 1\n",
    "            if word not in been:\n",
    "                ind = word_to_ind[word]\n",
    "                word_in_neg_text[ind] += 1\n",
    "                been.add(word)    \n",
    "        neg_words[word_to_ind[word]] += 1\n",
    "        cnt_neg += 1\n",
    "\n",
    "N_words = len(word_to_ind)\n",
    "N_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_popular(words_cnt, ind_to_word, top=30):\n",
    "    words = sorted(list(words_cnt.items()), key=lambda p:(p[1]))\n",
    "    top_words = words[-top:][::-1]\n",
    "    return pd.DataFrame({'word' : [ind_to_word[ind] for ind,cnt in top_words], 'cnt' : [cnt for ind,cnt in top_words]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular positive: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>105207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>96930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>87384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>54478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>50812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>46517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>40564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>34866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>30680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>it</td>\n",
       "      <td>23892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i</td>\n",
       "      <td>21922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>this</td>\n",
       "      <td>21059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>that</td>\n",
       "      <td>20735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"</td>\n",
       "      <td>19091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>as</td>\n",
       "      <td>16199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>with</td>\n",
       "      <td>14085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>for</td>\n",
       "      <td>13688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>was</td>\n",
       "      <td>13286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>but</td>\n",
       "      <td>12625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>film</td>\n",
       "      <td>12385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>movie</td>\n",
       "      <td>11585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>)</td>\n",
       "      <td>11549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(</td>\n",
       "      <td>11418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>his</td>\n",
       "      <td>10661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>on</td>\n",
       "      <td>10366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>he</td>\n",
       "      <td>9186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>are</td>\n",
       "      <td>9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>you</td>\n",
       "      <td>8834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>not</td>\n",
       "      <td>8643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word     cnt\n",
       "0     the  105207\n",
       "1       .   96930\n",
       "2       ,   87384\n",
       "3     and   54478\n",
       "4       a   50812\n",
       "5      of   46517\n",
       "6      to   40564\n",
       "7      is   34866\n",
       "8      in   30680\n",
       "9      it   23892\n",
       "10      i   21922\n",
       "11   this   21059\n",
       "12   that   20735\n",
       "13      -   19936\n",
       "14      \"   19091\n",
       "15     as   16199\n",
       "16   with   14085\n",
       "17    for   13688\n",
       "18    was   13286\n",
       "19    but   12625\n",
       "20   film   12385\n",
       "21  movie   11585\n",
       "22      )   11549\n",
       "23      (   11418\n",
       "24    his   10661\n",
       "25     on   10366\n",
       "26     he    9186\n",
       "27    are    9024\n",
       "28    you    8834\n",
       "29    not    8643"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Popular positive: \")\n",
    "find_popular(pos_words, ind_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popular negative: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>98927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>79786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>47958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>45050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>41743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>41641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>30346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>26473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>24527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i</td>\n",
       "      <td>24267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td>23779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>that</td>\n",
       "      <td>21304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>21032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"</td>\n",
       "      <td>20850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>was</td>\n",
       "      <td>15717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie</td>\n",
       "      <td>14684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>but</td>\n",
       "      <td>13117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>for</td>\n",
       "      <td>13113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>with</td>\n",
       "      <td>12671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>as</td>\n",
       "      <td>12436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>film</td>\n",
       "      <td>11426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>)</td>\n",
       "      <td>10366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>on</td>\n",
       "      <td>10276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(</td>\n",
       "      <td>10059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>not</td>\n",
       "      <td>9912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>you</td>\n",
       "      <td>9058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>have</td>\n",
       "      <td>9010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>be</td>\n",
       "      <td>8775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>are</td>\n",
       "      <td>8701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word     cnt\n",
       "0       .  101200\n",
       "1     the   98927\n",
       "2       ,   79786\n",
       "3       a   47958\n",
       "4     and   45050\n",
       "5      of   41743\n",
       "6      to   41641\n",
       "7      is   30346\n",
       "8      in   26473\n",
       "9    this   24527\n",
       "10      i   24267\n",
       "11     it   23779\n",
       "12   that   21304\n",
       "13      -   21032\n",
       "14      \"   20850\n",
       "15    was   15717\n",
       "16  movie   14684\n",
       "17    but   13117\n",
       "18    for   13113\n",
       "19   with   12671\n",
       "20     as   12436\n",
       "21   film   11426\n",
       "22      )   10366\n",
       "23     on   10276\n",
       "24      (   10059\n",
       "25    not    9912\n",
       "26    you    9058\n",
       "27   have    9010\n",
       "28     be    8775\n",
       "29    are    8701"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Popular negative: \")\n",
    "find_popular(neg_words, ind_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>neg_cnt</th>\n",
       "      <th>pos_cnt</th>\n",
       "      <th>bayes_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>24267</td>\n",
       "      <td>21922</td>\n",
       "      <td>-0.12769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gave</td>\n",
       "      <td>370</td>\n",
       "      <td>332</td>\n",
       "      <td>-0.13413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this</td>\n",
       "      <td>24527</td>\n",
       "      <td>21059</td>\n",
       "      <td>-0.17851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie</td>\n",
       "      <td>14684</td>\n",
       "      <td>11585</td>\n",
       "      <td>-0.26310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>47958</td>\n",
       "      <td>50812</td>\n",
       "      <td>0.03174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  neg_cnt  pos_cnt  bayes_weight\n",
       "0      i    24267    21922      -0.12769\n",
       "1   gave      370      332      -0.13413\n",
       "2   this    24527    21059      -0.17851\n",
       "3  movie    14684    11585      -0.26310\n",
       "4      a    47958    50812       0.03174"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_bayes_weights = []\n",
    "worst_bayes_weights = []\n",
    "all_words = set(pos_words.keys())\n",
    "for neg_w in neg_words.keys():\n",
    "    all_words.add(neg_w)\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "all_inds = list(all_words)\n",
    "all_words_neg = [neg_words[word] for word in all_inds]\n",
    "all_words_pos = [pos_words[word] for word in all_inds]\n",
    "all_words = [ind_to_word[ind] for ind in all_inds]\n",
    "\n",
    "d = {'word':all_words, 'neg_cnt':all_words_neg, 'pos_cnt':all_words_pos}\n",
    "\n",
    "stats = pd.DataFrame(data=d)\n",
    "\n",
    "V = len(all_words)\n",
    "\n",
    "stats['bayes_weight'] = np.log(((stats['pos_cnt'] + 1) / (cnt_pos + V)) / ((stats['neg_cnt'] + 1) / (cnt_neg + V)))\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST BAYES WEIGHTS STATS: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>neg_cnt</th>\n",
       "      <th>pos_cnt</th>\n",
       "      <th>bayes_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12160</th>\n",
       "      <td>edie</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>4.25060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19724</th>\n",
       "      <td>antwone</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>4.22243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6880</th>\n",
       "      <td>gundam</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>4.10107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13685</th>\n",
       "      <td>paulie</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>4.10107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23028</th>\n",
       "      <td>mildred</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>4.08481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>corbett</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>3.78060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10900</th>\n",
       "      <td>din</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>3.73513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27025</th>\n",
       "      <td>flavia</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>3.66281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>biko</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>3.61152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15156</th>\n",
       "      <td>deathtrap</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>3.55745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>trier</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>3.47044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15412</th>\n",
       "      <td>ossessione</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>3.43967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10899</th>\n",
       "      <td>gunga</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.40792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11331</th>\n",
       "      <td>brashear</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.40792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22019</th>\n",
       "      <td>yokai</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>3.40792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>mclaglen</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.37513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15427</th>\n",
       "      <td>visconti</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>3.37513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>gino</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3.34123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12435</th>\n",
       "      <td>daisies</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3.34123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15274</th>\n",
       "      <td>creasy</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3.34123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25438</th>\n",
       "      <td>iturbi</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3.34123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>sox</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>3.32384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>rea</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>3.30614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>venoms</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>3.30614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17912</th>\n",
       "      <td>ultimatum</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>3.30614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26428</th>\n",
       "      <td>warhols</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>3.30614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>tsui</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3.26977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16545</th>\n",
       "      <td>kells</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3.26977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25427</th>\n",
       "      <td>carface</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>3.26977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16222</th>\n",
       "      <td>blandings</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>3.23203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  neg_cnt  pos_cnt  bayes_weight\n",
       "12160        edie        0       71       4.25060\n",
       "19724     antwone        0       69       4.22243\n",
       "6880       gundam        0       61       4.10107\n",
       "13685      paulie        0       61       4.10107\n",
       "23028     mildred        0       60       4.08481\n",
       "5974      corbett        0       44       3.78060\n",
       "10900         din        0       42       3.73513\n",
       "27025      flavia        0       39       3.66281\n",
       "7571         biko        0       37       3.61152\n",
       "15156   deathtrap        0       35       3.55745\n",
       "3442        trier        0       32       3.47044\n",
       "15412  ossessione        0       31       3.43967\n",
       "10899       gunga        0       30       3.40792\n",
       "11331    brashear        0       30       3.40792\n",
       "22019       yokai        0       30       3.40792\n",
       "1149     mclaglen        0       29       3.37513\n",
       "15427    visconti        0       29       3.37513\n",
       "2539         gino        0       28       3.34123\n",
       "12435     daisies        0       28       3.34123\n",
       "15274      creasy        0       28       3.34123\n",
       "25438      iturbi        0       28       3.34123\n",
       "7196          sox        1       56       3.32384\n",
       "4478          rea        0       27       3.30614\n",
       "9895       venoms        0       27       3.30614\n",
       "17912   ultimatum        0       27       3.30614\n",
       "26428     warhols        0       27       3.30614\n",
       "5818         tsui        0       26       3.26977\n",
       "16545       kells        0       26       3.26977\n",
       "25427     carface        0       26       3.26977\n",
       "16222   blandings        0       25       3.23203"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"BEST BAYES WEIGHTS STATS: \")\n",
    "stats.nlargest(30, 'bayes_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова с большим наивным байсовским весом должны быть сильно позитивными.\\\n",
    "Из аномалий среди первых 30 слов можно выделить несколько слов: deathtrap - смертельная ловушка, ossessione-Обсессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORST BAYES WEIGHTS STATS: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>neg_cnt</th>\n",
       "      <th>pos_cnt</th>\n",
       "      <th>bayes_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52464</th>\n",
       "      <td>thunderbirds</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.68963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25310</th>\n",
       "      <td>boll</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.62338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50178</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.55243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52181</th>\n",
       "      <td>dahmer</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.49180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54270</th>\n",
       "      <td>beowulf</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.49180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25309</th>\n",
       "      <td>uwe</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.39336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58245</th>\n",
       "      <td>deathstalker</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.39336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56310</th>\n",
       "      <td>ajay</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.35827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52595</th>\n",
       "      <td>welch</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.32190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53779</th>\n",
       "      <td>tedium</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.32190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51981</th>\n",
       "      <td>kareena</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.28416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51879</th>\n",
       "      <td>turgid</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.24494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52852</th>\n",
       "      <td>hobgoblins</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.24494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35365</th>\n",
       "      <td>seagal</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.24494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55920</th>\n",
       "      <td>grendel</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.20412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58726</th>\n",
       "      <td>palermo</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.20412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61532</th>\n",
       "      <td>sarne</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.20412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18905</th>\n",
       "      <td>dreck</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.18307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43486</th>\n",
       "      <td>unwatchable</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.16156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52261</th>\n",
       "      <td>slater</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.16156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57911</th>\n",
       "      <td>kibbutz</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.16156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53213</th>\n",
       "      <td>maddy</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.11711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53305</th>\n",
       "      <td>kinski</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.11711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50051</th>\n",
       "      <td>stinker</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.10184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51767</th>\n",
       "      <td>shaq</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.07059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52104</th>\n",
       "      <td>lordi</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.07059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25163</th>\n",
       "      <td>ariel</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.04649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51941</th>\n",
       "      <td>domergue</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.02180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51980</th>\n",
       "      <td>saif</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.02180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52259</th>\n",
       "      <td>steaming</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.02180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  neg_cnt  pos_cnt  bayes_weight\n",
       "52464  thunderbirds       38        0      -3.68963\n",
       "25310          boll       72        1      -3.62338\n",
       "50178             <       67        1      -3.55243\n",
       "52181        dahmer       31        0      -3.49180\n",
       "54270       beowulf       31        0      -3.49180\n",
       "25309           uwe       57        1      -3.39336\n",
       "58245  deathstalker       28        0      -3.39336\n",
       "56310          ajay       27        0      -3.35827\n",
       "52595         welch       26        0      -3.32190\n",
       "53779        tedium       26        0      -3.32190\n",
       "51981       kareena       25        0      -3.28416\n",
       "51879        turgid       24        0      -3.24494\n",
       "52852    hobgoblins       24        0      -3.24494\n",
       "35365        seagal       74        2      -3.24494\n",
       "55920       grendel       23        0      -3.20412\n",
       "58726       palermo       23        0      -3.20412\n",
       "61532         sarne       23        0      -3.20412\n",
       "18905         dreck       46        1      -3.18307\n",
       "43486   unwatchable       68        2      -3.16156\n",
       "52261        slater       22        0      -3.16156\n",
       "57911       kibbutz       22        0      -3.16156\n",
       "53213         maddy       21        0      -3.11711\n",
       "53305        kinski       21        0      -3.11711\n",
       "50051       stinker       64        2      -3.10184\n",
       "51767          shaq       20        0      -3.07059\n",
       "52104         lordi       20        0      -3.07059\n",
       "25163         ariel       40        1      -3.04649\n",
       "51941      domergue       19        0      -3.02180\n",
       "51980          saif       19        0      -3.02180\n",
       "52259      steaming       19        0      -3.02180"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"WORST BAYES WEIGHTS STATS: \")\n",
    "stats.nsmallest(30, 'bayes_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова с маленьким наивным байсовским весом должны быть наоборот сильно отрицательными.\\\n",
    "Аномалий среди 30 последних по весу слов не наблюдается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)**\n",
    "### Реализовать байесовские классификаторы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Модель Бернулли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для векторизации текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokenized_texts, mode, bigrams=False):\n",
    "    global word_to_ind\n",
    "    res = [defaultdict(int) for _ in range(len(tokenized_texts))]\n",
    "    for ind, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word_to_ind:\n",
    "                res[ind][word_to_ind[token]] += 1\n",
    "    if bigrams:\n",
    "        for ind, text in enumerate(tokenized_texts):\n",
    "            bigrams = [b for b in zip(text[:-1], text[1:])]\n",
    "            for token in bigrams:\n",
    "                if token in word_to_ind:\n",
    "                    res[ind][word_to_ind[token]] += 1\n",
    "    if mode == 'bern':\n",
    "        for text in res:\n",
    "            for k, v in text.items():\n",
    "                if v > 0:\n",
    "                    text[k] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для подсчёта вероятности слова в классе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_word_cls(ind, cls, mode='bern'):\n",
    "    global word_in_pos_text, word_in_neg_text, word_to_ind, train_neg, train_pos, cnt_neg, cnt_pos, pos_words, neg_words\n",
    "    if mode == 'bern':\n",
    "        if cls == 'neg':\n",
    "            cnt = word_in_neg_text[ind] if ind != -1 else 0\n",
    "            return (1 + cnt) / (len(train_neg) + 2)\n",
    "        else:\n",
    "            cnt = word_in_pos_text[ind] if ind != -1 else 0\n",
    "            return (1 + cnt) / (len(train_pos) + 2)\n",
    "    elif mode == 'mult':\n",
    "        if cls == 'neg':\n",
    "            cnt = neg_words[ind] if ind != -1 else 0\n",
    "            return (1 + cnt) / (cnt_neg + len(word_to_ind))\n",
    "        else:\n",
    "            cnt = pos_words[ind] if ind != -1 else 0\n",
    "            return (1 + cnt) / (cnt_pos + len(word_to_ind))\n",
    "        \n",
    "# array with probability for each word\n",
    "prob_w_cls_bern = [[prob_word_cls(i, cls, mode='bern') for i in range(N_words)] for cls in ('neg', 'pos')]\n",
    "# логарифм вероятности, что в предложении нет ни одного слова\n",
    "bern_log_prob_empty = [sum(map(lambda x: log(1 - x), prob_w_cls_bern[i])) for i in (0, 1)]\n",
    "prob_w_cls_mult = [[prob_word_cls(i, cls, mode='mult') for i in range(N_words)] for cls in ('neg', 'pos')]\n",
    "# print(f\"CHECK PROBS: {sum(prob_w_cls_mult[0])}, {sum(prob_w_cls_mult[1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для вероятности класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_cls(cls):\n",
    "    global train_neg, train_pos\n",
    "    # 0 - neg, 1 - pos\n",
    "    if cls == 0:\n",
    "        return len(train_neg) / (len(train_neg) + len(train_pos))\n",
    "    else:\n",
    "        return len(train_pos) / (len(train_neg) + len(train_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Функция для классификации одного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_bern(vectorized_text):\n",
    "    # 0 - neg, 1 - pos\n",
    "    classes = (0, 1)\n",
    "    res = []\n",
    "    for cls in classes:\n",
    "        cur_res = bern_log_prob_empty[cls]\n",
    "        for ind, word in enumerate(vectorized_text.keys()):\n",
    "            cur_res -= log(1 - prob_w_cls_bern[cls][word])\n",
    "            cur_res += log(prob_w_cls_bern[cls][word])\n",
    "        cur_res += log(prob_cls(cls))\n",
    "        res.append(cur_res)\n",
    "    if res[0] > res[1]:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "    \n",
    "def classify_all(dataset, mode, return_probs=False):\n",
    "    #preprocessing\n",
    "    dataset = list(map(preprocessing, dataset))\n",
    "    dataset = tokenize_dataset(dataset)\n",
    "    dataset = vectorize(dataset, mode, bigrams=False)\n",
    "    probs = []\n",
    "    for i, text in enumerate(dataset):\n",
    "        if mode == 'bern':\n",
    "            tmp_pr = classify_bern(text)\n",
    "        else:\n",
    "            tmp_pr = classify_mult(text)\n",
    "        probs.append(tmp_pr)\n",
    "    return np.array(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "my_train_labels = classify_all(train_data, mode='bern')\n",
    "int_train_labels = [int(tag=='pos') for tag in train_labels]\n",
    "train_corr = 0\n",
    "for pair in zip(my_train_labels, int_train_labels):\n",
    "    train_corr += int(pair[0] == pair[1])\n",
    "\n",
    "train_acc = (train_corr) / len(int_train_labels)\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dev_texts_path, 'r', encoding='utf-8',) as inp:\n",
    "    dev_data = list(map(str.strip, inp.readlines()))\n",
    "with open(dev_labels_path, 'r', encoding='utf-8',) as inp:\n",
    "    dev_labels = list(map(str.strip, inp.readlines()))\n",
    " \n",
    "my_dev_labels = classify_all(dev_data, mode='bern')\n",
    "int_dev_labels = [int(tag=='pos') for tag in dev_labels]\n",
    "dev_corr = 0\n",
    "for pair in zip(my_dev_labels, int_dev_labels):\n",
    "    dev_corr += int(pair[0] == pair[1])\n",
    "# print(dev_corr)\n",
    "dev_acc = dev_corr / len(int_dev_labels)\n",
    "\n",
    "del dev_data\n",
    "del dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>dev_b_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9244</td>\n",
       "      <td>0.8523</td>\n",
       "      <td>0.595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  dev_acc  dev_b_acc\n",
       "1     0.9244   0.8523      0.595"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devb_acc = 1190/2000\n",
    "d = {'train_acc' : train_acc, 'dev_acc': dev_acc, 'dev_b_acc': devb_acc}\n",
    "accs_df = pd.DataFrame(index=[1], data = d)\n",
    "accs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные по dev_b датасету были взяты при тестировании классификатора через **evaluate.py**.\n",
    "Если в трейн_акк 0.99, то показываются результаты с биграммами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Мультиномиальная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этой модели нам нужно поменять функцию подсчёта вероятности встретить слово в классе.\\\n",
    "Это реализовано в виде режима в общей функции подсчёта этой вероятности.\n",
    "\n",
    "Теперь напишем функцию **classify_mult**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_mult(vectorized_text):\n",
    "    # 0 - neg, 1 - pos\n",
    "    classes = (0, 1)\n",
    "    res = []\n",
    "    for cls in classes:\n",
    "        cur_res = 0\n",
    "        for ind, word in enumerate(vectorized_text.keys()):\n",
    "            cur_res += log(prob_w_cls_mult[cls][word]) * vectorized_text[word]\n",
    "        cur_res += log(prob_cls(cls))\n",
    "        res.append(cur_res)\n",
    "    if res[0] > res[1]:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9148666666666667"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_labels = classify_all(train_data, mode='mult')\n",
    "int_train_labels = np.array([int(tag=='pos') for tag in train_labels])\n",
    "train_acc = ((my_train_labels == int_train_labels).sum() / len(int_train_labels))\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.846"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dev_texts_path, 'r', encoding='utf-8',) as inp:\n",
    "    dev_data = list(map(str.strip, inp.readlines()))\n",
    "with open(dev_labels_path, 'r', encoding='utf-8',) as inp:\n",
    "    dev_labels = list(map(str.strip, inp.readlines()))\n",
    "\n",
    "my_dev_labels = classify_all(dev_data, mode='mult')\n",
    "int_dev_labels = np.array([0 if tag=='neg' else 1 for tag in dev_labels])\n",
    "dev_acc = ((int_dev_labels == my_dev_labels).sum() / len(int_dev_labels))\n",
    "dev_corr = 0\n",
    "for pair in zip(my_dev_labels, int_dev_labels):\n",
    "    dev_corr += int(pair[0] == pair[1])\n",
    "dev_acc = dev_corr / len(int_dev_labels)\n",
    "del dev_data\n",
    "del dev_labels\n",
    "dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>dev_b_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.91487</td>\n",
       "      <td>0.846</td>\n",
       "      <td>74.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  dev_acc  dev_b_acc\n",
       "1    0.91487    0.846      74.45"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devb_acc = 0\n",
    "d = {'train_acc' : train_acc, 'dev_acc': dev_acc , 'dev_b_acc': 74.45}\n",
    "accs_df = pd.DataFrame(index=[1], data = d)\n",
    "accs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148666666666667 0.846\n"
     ]
    }
   ],
   "source": [
    "print(train_acc, dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные также взяты из результатов evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница в точности на тренировочной и остальных выборках происходит из-за нескольких причин:\\\n",
    "1. Другое распределение слов по позитивным и негативным классам.\n",
    "2. Слова, которые не встречаются в тестовой выборке встретились в других выборках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** Добавить в словарь биграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализации биграмм добавлены во все методы, тут будут только точности на выборках, потому что неудобно вставлять.\\\n",
    "Все цифры берутся из **evaluate.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель Бернулли + биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>dev_b_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.11</td>\n",
       "      <td>87.36</td>\n",
       "      <td>75.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  dev_acc  dev_b_acc\n",
       "1      99.11    87.36      75.85"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'train_acc' : 99.11, 'dev_acc': 87.36 , 'dev_b_acc': 75.85}\n",
    "pd.DataFrame(index=[1], data = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мультиномиальная модель + биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>dev_acc</th>\n",
       "      <th>dev_b_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.86</td>\n",
       "      <td>87.31</td>\n",
       "      <td>74.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  dev_acc  dev_b_acc\n",
       "1      98.86    87.31      74.85"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'train_acc' : 98.86, 'dev_acc': 87.31 , 'dev_b_acc': 74.85}\n",
    "pd.DataFrame(index=[1], data = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Триграммы появятся в research части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
