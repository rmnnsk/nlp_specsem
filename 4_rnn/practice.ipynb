{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Практическая чаcть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy \n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Загрузка датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cоставьте таблицу, в которой указано число токенов, уникальных токенов, предложений для каждой из трех частей датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = './filimdb_evaluation/PTB/'\n",
    "filenames = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>token_cnt</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>sentences_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>887521</td>\n",
       "      <td>9999</td>\n",
       "      <td>42068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "      <td>70390</td>\n",
       "      <td>6021</td>\n",
       "      <td>3370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>78669</td>\n",
       "      <td>6048</td>\n",
       "      <td>3761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>all files</td>\n",
       "      <td>1036580</td>\n",
       "      <td>9999</td>\n",
       "      <td>49199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file  token_cnt  unique_tokens  sentences_cnt\n",
       "0      train     887521           9999          42068\n",
       "1      valid      70390           6021           3370\n",
       "2       test      78669           6048           3761\n",
       "3  all files    1036580           9999          49199"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_info(filename):\n",
    "    global folder_name \n",
    "    tokens_cnt = defaultdict(int)\n",
    "    cnt_lines = 0\n",
    "    with open(folder_name + f\"ptb.{filename}.txt\", 'r') as inp:\n",
    "        for line in inp:\n",
    "            cnt_lines += 1\n",
    "            for token in line.strip().split():\n",
    "                tokens_cnt[token] += 1\n",
    "    total_tokens = sum(tokens_cnt.values())\n",
    "    unique_tokens = len(tokens_cnt.keys())\n",
    "    return filename, total_tokens, unique_tokens, cnt_lines, tokens_cnt\n",
    "\n",
    "data = {\n",
    "    'file':[],\n",
    "    'token_cnt':[],\n",
    "    'unique_tokens':[],\n",
    "    'sentences_cnt': []\n",
    "}\n",
    "\n",
    "file_dicts = []\n",
    "\n",
    "for f in filenames:\n",
    "    f_data = get_file_info(f)\n",
    "    data['file'].append(f_data[0])\n",
    "    data['token_cnt'].append(f_data[1])\n",
    "    data['unique_tokens'].append(f_data[2])\n",
    "    data['sentences_cnt'].append(f_data[3])\n",
    "    file_dicts.append(f_data[4])\n",
    "    \n",
    "all_tokens = defaultdict(int)\n",
    "for d in file_dicts:\n",
    "    for k, v in d.items():\n",
    "        all_tokens[k] += v\n",
    "\n",
    "data['file'].append('all files')\n",
    "data['token_cnt'].append(sum(data['token_cnt']))\n",
    "data['unique_tokens'].append(len(all_tokens.keys()))\n",
    "data['sentences_cnt'].append(sum(data['sentences_cnt']))\n",
    "    \n",
    "df = pd.DataFrame(data=data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Приведите 10 самых частотных и 10 самых редких токенов с их частотами.\n",
    "(тут видимо для всех файлов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_cnt = defaultdict(int)\n",
    "for f in filenames:\n",
    "     with open(folder_name + f\"ptb.{f}.txt\", 'r') as inp:\n",
    "        for line in inp:\n",
    "            for token in line.strip().split():\n",
    "                tokens_cnt[token] += 1\n",
    "cnt_list = list(tokens_cnt.items())\n",
    "cnt_list.sort(key=lambda x: x[1])\n",
    "most_frequent_data = {'word':[], 'cnt':[]}\n",
    "for w, c in cnt_list[-10:][::-1]:\n",
    "    most_frequent_data['word'].append(w)\n",
    "    most_frequent_data['cnt'].append(c)\n",
    "least_frequent_data = {'word':[], 'cnt':[]}\n",
    "for w, c in cnt_list[:10]:\n",
    "    least_frequent_data['word'].append(w)\n",
    "    least_frequent_data['cnt'].append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>59421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>53299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>37607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>of</td>\n",
       "      <td>28427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>27430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>24755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>in</td>\n",
       "      <td>21032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>and</td>\n",
       "      <td>20404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>'s</td>\n",
       "      <td>11555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>for</td>\n",
       "      <td>10436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word    cnt\n",
       "0    the  59421\n",
       "1  <unk>  53299\n",
       "2      N  37607\n",
       "3     of  28427\n",
       "4     to  27430\n",
       "5      a  24755\n",
       "6     in  21032\n",
       "7    and  20404\n",
       "8     's  11555\n",
       "9    for  10436"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=most_frequent_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>buffet</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>lancaster</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>barnett</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>rewrite</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>downgrading</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>backgrounds</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>stanza</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>vessel</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>unstable</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>peat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  cnt\n",
       "0       buffet    5\n",
       "1    lancaster    5\n",
       "2      barnett    5\n",
       "3      rewrite    5\n",
       "4  downgrading    5\n",
       "5  backgrounds    5\n",
       "6       stanza    5\n",
       "7       vessel    5\n",
       "8     unstable    5\n",
       "9         peat    5"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=least_frequent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Какие специальные токены уже есть в выборке, что они означают?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде как, токены выглядят как текст в треугольных кавычках. Поищем такие фрагменты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>'}\n"
     ]
    }
   ],
   "source": [
    "spec_tokens = set()\n",
    "for f in filenames:\n",
    "     with open(folder_name + f\"ptb.{f}.txt\", 'r') as inp:\n",
    "        for line in inp:\n",
    "            cur_spec = set(re.findall(r'<[a-z]*>', line))\n",
    "            spec_tokens = spec_tokens.union(cur_spec)\n",
    "print(spec_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что либо я перепутал тип токенов, либо у нас есть только один специальный токен : $<unk>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитав документацию по датасету, можно понять, что в нём содержатся 10000 самых популярных токенов, а все остальные токены заменяются на $<unk>$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Генерацей батчей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут написана версия разбиения на батчи для для слов в обычном виде, чтобы проще было проверить правильность построения. Сильно ниже будет версия генератора для уже приведенных к индексам слов в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch(ind, X_b, Y_b):\n",
    "    print(f\"Batch # {ind}\")\n",
    "    for i in range(len(X_b)):\n",
    "            print(X_b[i], ' ', Y_b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 51\n",
      "['pierre', '<unk>', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'n', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch'] 25\n",
      "['publishing', 'group', '<eos>', 'rudolph', '<unk>', 'n', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial'] 25\n",
      "Batch # 0\n",
      "['pierre', '<unk>', 'n']   ['<unk>', 'n', 'years']\n",
      "['publishing', 'group', '<eos>']   ['group', '<eos>', 'rudolph']\n",
      "Batch # 1\n",
      "['years', 'old', 'will']   ['old', 'will', 'join']\n",
      "['rudolph', '<unk>', 'n']   ['<unk>', 'n', 'years']\n",
      "Batch # 2\n",
      "['join', 'the', 'board']   ['the', 'board', 'as']\n",
      "['years', 'old', 'and']   ['old', 'and', 'former']\n",
      "Batch # 3\n",
      "['as', 'a', 'nonexecutive']   ['a', 'nonexecutive', 'director']\n",
      "['former', 'chairman', 'of']   ['chairman', 'of', 'consolidated']\n",
      "Batch # 4\n",
      "['director', 'nov.', 'n']   ['nov.', 'n', '<eos>']\n",
      "['consolidated', 'gold', 'fields']   ['gold', 'fields', 'plc']\n",
      "Batch # 5\n",
      "['<eos>', 'mr.', '<unk>']   ['mr.', '<unk>', 'is']\n",
      "['plc', 'was', 'named']   ['was', 'named', 'a']\n",
      "Batch # 6\n",
      "['is', 'chairman', 'of']   ['chairman', 'of', '<unk>']\n",
      "['a', 'nonexecutive', 'director']   ['nonexecutive', 'director', 'of']\n",
      "Batch # 7\n",
      "['<unk>', 'n.v.', 'the']   ['n.v.', 'the', 'dutch']\n",
      "['of', 'this', 'british']   ['this', 'british', 'industrial']\n",
      "Batch # 8\n"
     ]
    }
   ],
   "source": [
    "def batch_generator_text(data_path, batch_size, num_steps, debug=False):\n",
    "    eos_token = '<eos>'\n",
    "    L_tokens = []\n",
    "    with open(data_path, 'r', encoding='utf-8') as inp:\n",
    "        for line in inp:\n",
    "            line_tokens = list(map(str.lower, line.strip().split()))\n",
    "            L_tokens.extend(line_tokens + [eos_token])\n",
    "    L_shifted = L_tokens[1:]\n",
    "    L_tokens = L_tokens[:-1]\n",
    "    print(len(L_tokens), len(L_shifted))\n",
    "    unk_len = len(L_tokens) // batch_size\n",
    "    X_lists = [L_tokens[i : i + unk_len] for i in range(0, len(L_tokens), unk_len) if len(L_tokens[i : i + unk_len]) == unk_len]\n",
    "    Y_lists = [L_shifted[i : i + unk_len] for i in range(0, len(L_shifted), unk_len)  if len(L_shifted[i : i + unk_len]) == unk_len] \n",
    "    \n",
    "    for lst in X_lists:\n",
    "        print(lst, len(lst))\n",
    "    for i in range(0, unk_len, num_steps):\n",
    "        \n",
    "        X_batch = []\n",
    "        Y_batch = []\n",
    "        for lst in X_lists:\n",
    "            if len(lst[i : i + num_steps]) == num_steps:\n",
    "                X_batch.append(lst[i : i + num_steps])\n",
    "        for lst in Y_lists:\n",
    "            if len(lst[i : i + num_steps]) == num_steps:\n",
    "                Y_batch.append(lst[i : i + num_steps])\n",
    "        if debug:\n",
    "            print_batch(i // num_steps, X_batch, Y_batch)\n",
    "    \n",
    "batch_generator_text(folder_name + \"small.txt\", batch_size = 2, num_steps = 3, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На файле из первых трех строчек **train** датасета функция создаёт батчи похожие на правду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Реализация LSTM LM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Класс LSTMCell\n",
    "Для реализации LSTM ячейки будем отталкиваться от реализации обычной RNN ячейки из семинара."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        '''\n",
    "        Args:\n",
    "            input_size: Size of token embedding\n",
    "            hidden_size: Size of hidden state of LSTM cell\n",
    "        '''\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Creating matrices whose weights will be trained\n",
    "        # Token embedding (input of this cell) will be multiplied by this matrix\n",
    "        self.U_input = torch.nn.Parameter(torch.Tensor(input_size, 4 * hidden_size))\n",
    "        self.BU_input = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "\n",
    "        # Creating matrices whose weights will be trained\n",
    "        # Hidden state from previous step will be multipied by this matrix\n",
    "        # Zero hidden state at the initial step\n",
    "        self.W_hidden = torch.nn.Parameter(torch.Tensor(hidden_size, 4 * hidden_size))\n",
    "        self.BW_hidden = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n",
    "\n",
    "        # Weights initialization\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, inp: torch.Tensor, cell_state: torch.Tensor, hidden_state: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        '''\n",
    "        Performes forward pass of the recurrent cell\n",
    "        Args:\n",
    "            inp: Output from Embedding layer at the current timestep\n",
    "                Tensor shape is (batch_size, emb_size)\n",
    "            cell_state: Output cell_state from previous recurrent step or zero state\n",
    "                Tensor shape is (batch_size, hidden_size)\n",
    "            hidden_state: Output hidden_state from previous recurrent step or zero state\n",
    "                Tensor shape is (batch_size, hidden_size)\n",
    "        Returns:\n",
    "            Output from LSTM cell\n",
    "        '''\n",
    "        hidden_mult = hidden_state @ self.W_hidden + self.BW_hidden\n",
    "        input_mult  = inp @ self.U_input + self.BU_input \n",
    "        matr_sum = input_mult + hidden_mult\n",
    "        \n",
    "        sum_chunk = matr_sum.chunk(chunks=4, dim=1)\n",
    "        f, i, o = torch.sigmoid(sum_chunk[0]), torch.sigmoid(sum_chunk[1]), torch.sigmoid(sum_chunk[3])\n",
    "        c_new = torch.tanh(sum_chunk[2])\n",
    "        \n",
    "        cell_state_new = cell_state * f + i * c_new\n",
    "        hidden_state_new = o * torch.tanh(cell_state_new)\n",
    "        \n",
    "        return cell_state_new, hidden_state_new\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        '''\n",
    "        Weights initialization\n",
    "        '''\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 матриц и векторов смещений заменили на 2 каждого вида.<br>\n",
    "Всё перемножили и сложили по формулам, применили функции активация к каждой из 4 частей большой матрицы. <br>\n",
    "Дальше осталось просто всё правильно поэлементно перемножить и получить новые состояния ячейки и скрытое состояние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Класс LSTMLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(torch.nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.input_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.LSTMCell = LSTMCell(emb_size, hidden_size)\n",
    "        \n",
    "    def forward(self, X_batch, initial_states=None):\n",
    "        if initial_states is None:\n",
    "            cell_state = torch.zeros((X_batch.shape[1], self.hidden_size))\n",
    "            hidden_state = torch.zeros((X_batch.shape[1], self.hidden_size))\n",
    "        else:\n",
    "            cell_state, hidden_state = initial_states\n",
    "            \n",
    "        #X_batch.shape = (num_steps, batch_size, emb_size)\n",
    "        #Need to transform this somewhere else in LSTM class\n",
    "        \n",
    "        outputs = []\n",
    "#         print(f\"LSTMLayer: X_batch.shape={X_batch.shape}, cell_state.shape={cell_state.shape}, hidden_state.shape={hidden_state.shape}\")\n",
    "        for timestamp in range(X_batch.shape[0]):\n",
    "            cell_state, hidden_state = self.LSTMCell(X_batch[timestamp], cell_state, hidden_state)\n",
    "            outputs.append(hidden_state)\n",
    "        \n",
    "        #outputs - list of tensors with shape (batch_size, hidden_size)\n",
    "        return torch.stack(outputs), (cell_state, hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Класс LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(LSTMLayer(emb_size, hidden_size))\n",
    "            else:\n",
    "                self.layers.append(LSTMLayer(hidden_size, hidden_size))\n",
    "            self.layers.append(torch.nn.Dropout(p=self.dropout_rate))\n",
    "        self.layers.pop()\n",
    "            \n",
    "    def forward(self, X_batch):\n",
    "        for ind, layer in enumerate(self.layers):\n",
    "            if ind % 2 == 0:\n",
    "#                 print(f\"LSTM: layer # {ind + 1}\")\n",
    "                X_batch, _ = layer(X_batch)\n",
    "            else:\n",
    "#                 print(f\"LSTM: dropout\")\n",
    "                X_batch = layer(X_batch)\n",
    "        return X_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Класс PTBLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBLM(torch.nn.Module):\n",
    "    def __init__(self, num_layers, emb_size, hidden_size, vocab_size, dropout_rate):\n",
    "        super(PTBLM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "        self.LSTM = LSTM(emb_size, hidden_size, num_layers, dropout_rate)\n",
    "        self.decoder = torch.nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, model_input):\n",
    "        #embs.shape = (num_steps, batch_size, emb_shape)\n",
    "        embs = self.embedding(model_input).transpose(0, 1).contiguous()\n",
    "        \n",
    "#         print (\"PTBLM: embs.shape = \", embs.shape)\n",
    "        \n",
    "        outputs = self.LSTM(embs)\n",
    "        \n",
    "        logits = self.decoder(outputs).transpose(0, 1).contiguous()\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -1.0, 1.0)\n",
    "        torch.nn.init.uniform_(self.decoder.weight, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Обучение языковой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже функции для подготовки ptb датасета и словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN =  '<start>'\n",
    "EOS_TOKEN =  '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pierre', '<unk>', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'n', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', 'rudolph', '<unk>', 'n', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate']\n"
     ]
    }
   ],
   "source": [
    "def _read_words(path):\n",
    "    with open(path, 'r') as inp:\n",
    "        names = inp.read().lower().split()\n",
    "        return names\n",
    "print(_read_words(folder_name + 'small.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'pierre', '<unk>', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'n']\n",
      "['<start>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group']\n",
      "['<start>', 'rudolph', '<unk>', 'n', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate']\n"
     ]
    }
   ],
   "source": [
    "def _read_sentences(path):\n",
    "    with open(path, 'r') as inp:\n",
    "        sentences = inp.read().lower().split('\\n')\n",
    "    sentences = [[START_TOKEN] + sent.split() for sent in sentences]\n",
    "    return sentences\n",
    "    \n",
    "sents = _read_sentences(folder_name + 'small.txt')\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size =  37\n",
      "[('<unk>', 0), ('n', 1), ('of', 2), ('years', 3), ('old', 4), ('the', 5), ('a', 6), ('nonexecutive', 7), ('director', 8), ('chairman', 9), ('pierre', 10), ('will', 11), ('join', 12), ('board', 13), ('as', 14), ('nov.', 15), ('mr.', 16), ('is', 17), ('n.v.', 18), ('dutch', 19), ('publishing', 20), ('group', 21), ('rudolph', 22), ('and', 23), ('former', 24), ('consolidated', 25), ('gold', 26), ('fields', 27), ('plc', 28), ('was', 29), ('named', 30), ('this', 31), ('british', 32), ('industrial', 33), ('conglomerate', 34), ('<start>', 35), ('<eos>', 36)]\n"
     ]
    }
   ],
   "source": [
    "def _build_vocab(path):\n",
    "    data = _read_words(path)\n",
    "    special_tokens = [START_TOKEN, EOS_TOKEN]\n",
    "    data += special_tokens\n",
    "    \n",
    "    counter = Counter(data)\n",
    "    sorted_words = sorted(counter.items(), key=lambda x: -x[1])\n",
    "    \n",
    "    words = [w for w, _ in sorted_words]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "    \n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "word_to_id, id_to_word = _build_vocab(folder_name + 'small.txt')\n",
    "print('Vocab size = ', len(word_to_id))\n",
    "print(list(word_to_id.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 10, 0, 1, 3, 4, 11, 12, 5, 13, 14, 6, 7, 8, 15, 1]\n",
      "[35, 16, 0, 17, 9, 2, 0, 18, 5, 19, 20, 21]\n",
      "[35, 22, 0, 1, 3, 4, 23, 24, 9, 2, 25, 26, 27, 28, 29, 30, 6, 7, 8, 2, 31, 32, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "def _sentences_to_word_ids(path, word_to_id):\n",
    "    sentences = _read_sentences(path)\n",
    "    return [[word_to_id[word] for word in sent] for sent in sentences]\n",
    "\n",
    "word_to_id, id_to_word = _build_vocab(folder_name + 'small.txt')\n",
    "res = _sentences_to_word_ids(folder_name + 'small.txt', word_to_id)\n",
    "for sent in res:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size =  10001\n",
      "[9999, 9969, 9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992]\n",
      "[9999, 8568, 1, 2, 71, 392, 32, 2115, 0, 145, 18, 5, 8569, 274, 406, 2]\n",
      "[9999, 22, 1, 12, 140, 3, 1, 5277, 0, 3054, 1580, 95]\n",
      "[9999, 7231, 1, 2, 71, 392, 7, 336, 140, 3, 2467, 656, 2157, 948, 23, 520, 5, 8569, 274, 3, 38, 302, 436, 3660]\n",
      "[9999, 5, 940, 3, 3142, 494, 261, 4, 136, 5881, 4218, 5882, 29, 985, 5, 239, 754, 3, 1012, 2764, 210, 5, 95, 3, 426, 4059, 4, 13, 44, 54, 2, 71, 194, 1232, 219]\n"
     ]
    }
   ],
   "source": [
    "def ptb_raw_data(data_path):\n",
    "    train_path = os.path.join(data_path, 'ptb.train.txt')\n",
    "    dev_path = os.path.join(data_path, 'ptb.valid.txt')\n",
    "    test_path = os.path.join(data_path, 'ptb.test.txt')\n",
    "    \n",
    "    word_to_id, id_to_word = _build_vocab(train_path)\n",
    "    train_data = _sentences_to_word_ids(train_path, word_to_id)\n",
    "    dev_data = _sentences_to_word_ids(dev_path, word_to_id)\n",
    "    test_data = _sentences_to_word_ids(test_path, word_to_id)\n",
    "    \n",
    "    return train_data, dev_data, test_data, word_to_id, id_to_word\n",
    "\n",
    "train_data, dev_data, test_data, word_to_ind, ind_to_word = ptb_raw_data(folder_name)\n",
    "print('Vocab size = ', len(word_to_ind))\n",
    "for sent in train_data[:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_inds(data, word_to_id, batch_size, num_steps, debug=False):\n",
    "    L_tokens = []\n",
    "    for sentence in data:\n",
    "        L_tokens.extend(sentence + [word_to_ind[EOS_TOKEN]])\n",
    "    L_shifted = L_tokens[1:]\n",
    "    L_tokens = L_tokens[:-1]\n",
    "    \n",
    "    unk_len = len(L_tokens) // batch_size\n",
    "    X_lists = [L_tokens[i : i + unk_len] for i in range(0, len(L_tokens), unk_len) if len(L_tokens[i : i + unk_len]) == unk_len]\n",
    "    Y_lists = [L_shifted[i : i + unk_len] for i in range(0, len(L_shifted), unk_len)  if len(L_shifted[i : i + unk_len]) == unk_len] \n",
    "       \n",
    "    for i in range(0, unk_len, num_steps):\n",
    "        \n",
    "        X_batch = []\n",
    "        Y_batch = []\n",
    "        for lst in X_lists:\n",
    "            if len(lst[i : i + num_steps]) == num_steps:\n",
    "                X_batch.append(lst[i : i + num_steps])\n",
    "        for lst in Y_lists:\n",
    "            if len(lst[i : i + num_steps]) == num_steps:\n",
    "                Y_batch.append(lst[i : i + num_steps])\n",
    "        if debug:\n",
    "            print_batch(i // num_steps, X_batch, Y_batch)\n",
    "        else:\n",
    "            yield torch.tensor(X_batch), torch.tensor(Y_batch)\n",
    "\n",
    "train_data, dev_data, test_data, word_to_ind, ind_to_word = ptb_raw_data(folder_name)\n",
    "res = batch_generator_inds(train_data, word_to_ind, batch_size = 2, num_steps = 3, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к функциям для обучения сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(optimizer, lr):\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "def run_epoch(\n",
    "    lr,\n",
    "    model,\n",
    "    data, \n",
    "    word_to_id, \n",
    "    loss_fn, \n",
    "    batch_size,\n",
    "    num_steps,\n",
    "    optimizer = None, \n",
    "    device = None\n",
    ") -> float:\n",
    "    '''\n",
    "    Performs one training epoch or inference epoch\n",
    "    Args:\n",
    "        lr: Learning rate for this epoch\n",
    "        model: Language model object\n",
    "        data: Data that will be passed through the language model\n",
    "        char_to_id: Mapping of each character into its index in the vocabulary\n",
    "        loss_fn: Torch loss function\n",
    "        optimizer: Torch optimizer\n",
    "        device: Input tensors should be sent to this device\n",
    "    Returns: \n",
    "        Perplexity\n",
    "    '''\n",
    "    total_loss, total_examples = 0.0, 0\n",
    "    generator = batch_generator_inds(data, word_to_id=word_to_id, batch_size=batch_size, num_steps=num_steps)\n",
    "\n",
    "    for step, (X, Y) in enumerate(generator):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        if optimizer is not None:\n",
    "            # After each call of loss.backward() gradients are accumulated, \n",
    "            # so, .zero_grad() clears the gradients of all optimized torch.Tensor\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # initial_state.shape == (batch_size, hidden_size)\n",
    "        # If you use LSTM cell you will need two zero tensors for each recurrent layer\n",
    "        \n",
    "        # logits.shape = (batch_size, seq_len, vocab_size)\n",
    "        logits = model(X)\n",
    "\n",
    "        # Loss function input - two tensors:\n",
    "        #     1. Distribution over vocabulary - two-dimensional tensor (logits has 3 dimensions)\n",
    "        #        Should be a tensor of the following size (batch_size * seq_len, vocab_size)\n",
    "        #        So, logits.view((-1, model.vocab_size))\n",
    "        #     2. Target labels - one-dimensional tensor, so Y.view(-1)\n",
    "        loss = loss_fn(logits.view((-1, model.vocab_size)), Y.view(-1))\n",
    "        total_examples += loss.size(0)\n",
    "        total_loss += loss.sum().item()\n",
    "        loss = loss.mean()\n",
    "\n",
    "        if optimizer is not None:\n",
    "            # We have a new learning rate value at every step, so it needs to be updated\n",
    "            update_lr(optimizer, lr)\n",
    "            \n",
    "            # Gradients computation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping by predefined norm value - usually 5.0\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
    "\n",
    "            # Applying gradients - one gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "    return torch.exp(total_loss / total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab size will be init later\n",
    "config = { 'batch_size': 64, 'num_steps': 35, \n",
    "           'num_layers': 2, 'emb_size': 256,\n",
    "           'hidden_size': 256, 'vocab_size': -1,\n",
    "           'dropout_rate': 0.2, 'num_epochs': 13,\n",
    "           'learning_rate': 0.01, 'lr_decay' : 0.9,\n",
    "           'epoch_decay' : 6\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = ptb_raw_data(folder_name)\n",
    "train_data, dev_data, test_data, word_to_id, id_to_word = raw_data\n",
    "config['vocab_size'] = len(word_to_id)\n",
    "config['vocab_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTBLM(\n",
       "  (embedding): Embedding(10001, 256)\n",
       "  (LSTM): LSTM()\n",
       "  (decoder): Linear(in_features=256, out_features=10001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PTBLM(num_layers=config['num_layers'], emb_size=config['emb_size'],\n",
    "              hidden_size=config['hidden_size'], vocab_size = config['vocab_size'],\n",
    "              dropout_rate=config['dropout_rate']\n",
    "             )\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "наконец-то цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0982b9061b054d599d8094e6a79df147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data = []\n",
    "for i in trange(config['num_epochs']):\n",
    "    lr_decay = config['lr_decay'] ** max(i + 1 - config['epoch_decay'], 0.0)\n",
    "    decayed_lr = config['learning_rate'] * lr_decay\n",
    "    \n",
    "    model.train()\n",
    "    train_perplexity = run_epoch(decayed_lr, model, train_data, \n",
    "                                 word_to_id, loss_fn,\n",
    "                                 config['batch_size'], config['num_steps'],\n",
    "                                 optimizer=optimizer, \n",
    "                                 device=device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Disabling gradient calculation. \n",
    "    # It will reduce memory consumption for computations \n",
    "    # The result of every computation will have requires_grad=False, \n",
    "    with torch.no_grad():\n",
    "        dev_perplexity = run_epoch(decayed_lr, model, dev_data, \n",
    "                                   word_to_id, loss_fn, config['batch_size'], config['num_steps'],\n",
    "                                   device=device)\n",
    "    \n",
    "    plot_data.append((i, train_perplexity, dev_perplexity, decayed_lr))\n",
    "    print(f'Epoch: {i+1}. Learning rate: {decayed_lr:.3f}. '\n",
    "          f'Train Perplexity: {train_perplexity:.3f}. '\n",
    "          f'Dev Perplexity: {dev_perplexity:.3f}. ' \n",
    "          f'Generated Names: {names}.\\n')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_perplexity = run_epoch(\n",
    "        decayed_lr, model, test_data, \n",
    "        word_to_id, loss_fn, config['batch_size'], config['num_steps'],\n",
    "        device=device)\n",
    "    print(f\"Test Perplexity: {test_perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
